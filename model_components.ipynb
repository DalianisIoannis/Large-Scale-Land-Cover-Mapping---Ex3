{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code comes from https://github.com/jnyborg/timematch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from torchvision.transforms import transforms\n",
    "from utils import RandomSamplePixels, Normalize, ToTensor, PixelSetData,\\\n",
    "    split_dict_train_test, pad_sequences_collate_fn, LinearLayer, PixelSetEncoder,\\\n",
    "        PositionalEncoding, TimeSeriesTransformer, SimpleMLP\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([52, 32, 10])\n",
      "torch.Size([52, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "# pixels_tmp = pixel_dataset[1901]['pixels'].permute(0, 2, 1)\n",
    "pixels_tmp = torch.randn(52, 10, 32).permute(0, 2, 1)\n",
    "print(\"input\", pixels_tmp.shape)\n",
    "\n",
    "ll = LinearLayer(10, 64)\n",
    "\n",
    "print(ll(pixels_tmp).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δοκιμή του LinearLayer με τυχαία διανύσματα όμως ΧΩΡΙΣ μάσκες"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): LinearLayer(\n",
      "    (linear): Linear(in_features=10, out_features=32, bias=False)\n",
      "    (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): LinearLayer(\n",
      "    (linear): Linear(in_features=64, out_features=128, bias=False)\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 52, 10, 32])\n",
      "torch.Size([52, 32, 10])\n",
      "torch.Size([52, 32, 32])\n",
      "torch.Size([52, 64])\n",
      "torch.Size([52, 128])\n",
      "torch.Size([1, 52, 128])\n"
     ]
    }
   ],
   "source": [
    "# mlp1_dim=[10, 32, 64]\n",
    "mlp1_dim=[10, 32]\n",
    "mlp2_dim=[64, 128]\n",
    "\n",
    "layers = []\n",
    "for i in range(len(mlp1_dim) - 1):\n",
    "    layers.append(LinearLayer(mlp1_dim[i], mlp1_dim[i + 1]))\n",
    "mlp1 = nn.Sequential(*layers)\n",
    "print(mlp1)\n",
    "\n",
    "layers = []\n",
    "for i in range(len(mlp2_dim) - 1):\n",
    "    layers.append(LinearLayer(mlp2_dim[i], mlp2_dim[i + 1]))\n",
    "mlp2 = nn.Sequential(*layers)\n",
    "print(mlp2)\n",
    "\n",
    "# out = pixel_dataset[1901]['pixels'].unsqueeze(0)\n",
    "out = torch.randn(52, 10, 32).unsqueeze(0)\n",
    "print(out.shape) # torch.Size([1, 52, 10, 32])\n",
    "\n",
    "batch, temp = out.shape[:2]\n",
    "# print(batch, temp) # 1 52\n",
    "\n",
    "out = out.view(batch * temp, *out.shape[2:]).transpose(1, 2)  # (B*T, S, C)\n",
    "print(out.shape) # torch.Size([52, 32, 10])\n",
    "\n",
    "out = mlp1(out).transpose(1, 2)\n",
    "print(out.shape) # torch.Size([52, 32, 32])\n",
    "\n",
    "out = torch.cat(\n",
    "            [out.mean(dim=-1), out.std(dim=-1)],\n",
    "            dim=1)\n",
    "print(out.shape) # torch.Size([52, 64])\n",
    "\n",
    "out = mlp2(out)\n",
    "print(out.shape) # torch.Size([52, 128])\n",
    "\n",
    "out = out.view(batch, temp, -1)\n",
    "print(out.shape) # torch.Size([1, 52, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSE Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Οι extra χωρικές πληροφορίες δε χρειάζονται"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δημιουργία Dataset και Dataloaders για τη δοκιμή του PSE και του Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4255 7\n",
      "0 corn\n",
      "1 corn\n",
      "2 corn\n",
      "3 corn\n",
      "4 corn\n",
      "5 spring_barley\n",
      "\n",
      "corn 275\n",
      "spring_barley 1141\n",
      "meadow 1013\n",
      "winter_wheat 856\n",
      "winter_rapeseed 301\n",
      "winter_barley 352\n",
      "winter_rye 317\n"
     ]
    }
   ],
   "source": [
    "# labels_200 will contain the labels with more than 200 occurrences\n",
    "f_labels = open(r\"Exercise4\\timematch_data\\denmark\\32VNH\\2017\\meta\\labels_cleaned.json\")\n",
    "labels_200 = json.load(f_labels)\n",
    "\n",
    "print(len(labels_200), len(set(labels_200.values())))\n",
    "\n",
    "count = 0\n",
    "for lab in labels_200:\n",
    "    print(lab, labels_200[lab])\n",
    "    count += 1\n",
    "    if count == 6:\n",
    "        break\n",
    "\n",
    "labels_200_counter = Counter(labels_200.values())\n",
    "\n",
    "print()\n",
    "\n",
    "for lab in labels_200_counter:\n",
    "    print(lab, labels_200_counter[lab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Οι παρακάτω αντιστοιχίσεις είναι διαφορετικές από του dataset_creation αλλά δεν πειράζει"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corn 0\n",
      "spring_barley 1\n",
      "meadow 2\n",
      "winter_wheat 3\n",
      "winter_rapeseed 4\n",
      "winter_barley 5\n",
      "winter_rye 6\n",
      "\n",
      "4255\n",
      "3371\n",
      "849\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "class_to_idx = {cls: idx for idx, cls in enumerate(labels_200_counter)}\n",
    "for key, val in class_to_idx.items():\n",
    "    print(key, val)\n",
    "\n",
    "print()\n",
    "\n",
    "# train_labels, val_labels = split_dict_train_test(labels_200, test_size=0.2)\n",
    "train_labels, val_labels, test_labels = split_dict_train_test(labels_200, test_size=0.007, val_size=0.2)\n",
    "\n",
    "print(len(labels_200))\n",
    "print(len(train_labels))\n",
    "print(len(val_labels))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3371\n",
      "\n",
      "849\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([RandomSamplePixels(32), Normalize(), ToTensor()])\n",
    "\n",
    "test_transform = transforms.Compose([Normalize(), ToTensor()])\n",
    "\n",
    "train_dataset = PixelSetData(\"Exercise4/timematch_data/denmark/32VNH/2017\",\n",
    "                             class_to_idx, train_labels,\n",
    "                             train_transform)\n",
    "print(len(train_dataset))\n",
    "print()\n",
    "val_dataset = PixelSetData(\"Exercise4/timematch_data/denmark/32VNH/2017\",\n",
    "                             class_to_idx, val_labels,\n",
    "                             test_transform)\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "107\n",
      "torch.Size([64, 52, 10, 32])\n",
      "torch.Size([64])\n",
      "tensor([2, 3, 1, 4, 2, 1, 2, 3, 3, 2, 3, 4, 1, 6, 2, 2, 6, 1, 6, 2, 2, 6, 3, 0,\n",
      "        6, 0, 1, 3, 0, 1, 6, 3, 2, 5, 5, 1, 1, 4, 3, 1, 5, 2, 1, 3, 2, 2, 3, 1,\n",
      "        3, 5, 5, 0, 2, 1, 1, 6, 4, 2, 3, 1, 1, 1, 6, 3])\n",
      "torch.Size([64, 52, 32])\n",
      "[ True]\n",
      "\n",
      "torch.Size([8, 52, 10, 3526])\n",
      "torch.Size([8])\n",
      "tensor([6, 1, 2, 4, 2, 1, 2, 4])\n",
      "torch.Size([8, 52, 3526])\n",
      "[False  True]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dloader = DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True,\n",
    "    collate_fn=pad_sequences_collate_fn,\n",
    "    # num_workers=4,\n",
    ")\n",
    "val_dloader = DataLoader(\n",
    "    val_dataset, batch_size=8, shuffle=True,\n",
    "    collate_fn=pad_sequences_collate_fn,\n",
    ")\n",
    "\n",
    "print(len(train_dloader))\n",
    "print(len(val_dloader))\n",
    "\n",
    "for batch in train_dloader:\n",
    "    print(batch[0].shape)\n",
    "    print(batch[1].shape)\n",
    "    print(batch[1])\n",
    "    print(batch[2].shape)\n",
    "    print(np.unique(batch[2]))\n",
    "    print()\n",
    "    break\n",
    "\n",
    "for batch in val_dloader:\n",
    "    print(batch[0].shape)\n",
    "    print(batch[1].shape)\n",
    "    print(batch[1])\n",
    "    print(batch[2].shape)\n",
    "    print(np.unique(batch[2]))\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixelSetEncoder(\n",
      "  (mlp1): Sequential(\n",
      "    (0): LinearLayer(\n",
      "      (linear): Linear(in_features=10, out_features=32, bias=False)\n",
      "      (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (mlp2): Sequential(\n",
      "    (0): LinearLayer(\n",
      "      (linear): Linear(in_features=64, out_features=128, bias=False)\n",
      "      (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (activation): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "torch.Size([64, 52, 10, 32])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 52, 32])\n",
      "\n",
      "torch.Size([64, 52, 128])\n",
      "\n",
      "[0.0000000e+00 1.0728836e-05 1.5757978e-05 ... 9.5031271e+00 9.6956177e+00\n",
      " 9.7326565e+00]\n"
     ]
    }
   ],
   "source": [
    "pixel_set_encoder = PixelSetEncoder(10, mlp1=[10, 32],\n",
    "                                    pooling=\"mean_std\",\n",
    "                                    mlp2=[64, 128])\n",
    "print(pixel_set_encoder)\n",
    "print()\n",
    "\n",
    "for batch in train_dloader:\n",
    "    input_tensor = batch[0]\n",
    "    labels = batch[1]\n",
    "    mask = batch[2]\n",
    "\n",
    "    print(input_tensor.shape)\n",
    "    print(labels.shape)\n",
    "    print(mask.shape)\n",
    "\n",
    "    break\n",
    "\n",
    "print()\n",
    "\n",
    "out = pixel_set_encoder(input_tensor, mask)\n",
    "print(out.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(np.unique(out.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 52, 128])\n"
     ]
    }
   ],
   "source": [
    "# can also check Tsironis\n",
    "pe = PositionalEncoding(128)\n",
    "\n",
    "input = out\n",
    "out = pe(input)\n",
    "\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesTransformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-2): 3 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pos_emb): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Output shape:  torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "# time_series_transformer = TimeSeriesTransformer(feature_size=128)\n",
    "time_series_transformer = TimeSeriesTransformer(d_model=128)\n",
    "print(time_series_transformer)\n",
    "\n",
    "# input = torch.randn(1, 52, 128)\n",
    "input = out\n",
    "out = time_series_transformer(input)\n",
    "print(\"Output shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMLP(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 7])\n",
      "tensor([[-0.1746,  0.2504, -0.0499,  0.5450,  0.8937, -0.2919, -0.3470],\n",
      "        [-0.1821,  0.2377,  0.1150,  0.5664,  0.9340, -0.3612, -0.2917],\n",
      "        [-0.2063,  0.4163,  0.0055,  0.5068,  0.9041, -0.4562, -0.3397],\n",
      "        [-0.1338,  0.2349, -0.1106,  0.6577,  0.9399, -0.3407, -0.4588],\n",
      "        [-0.1590,  0.3552,  0.0870,  0.7250,  0.9227, -0.3058, -0.3813],\n",
      "        [-0.1585,  0.3536,  0.1522,  0.6340,  0.8709, -0.3371, -0.2816],\n",
      "        [-0.2002,  0.2721,  0.1608,  0.6488,  0.7744, -0.3228, -0.3664],\n",
      "        [-0.2745,  0.4704, -0.2115,  0.7547,  0.8209, -0.3990, -0.3841],\n",
      "        [-0.2035,  0.3238,  0.0669,  0.5581,  0.7886, -0.2964, -0.4057],\n",
      "        [-0.1145,  0.2535, -0.0115,  0.6677,  0.8945, -0.2676, -0.4759],\n",
      "        [-0.1480,  0.2993,  0.0103,  0.6005,  0.8956, -0.3908, -0.2354],\n",
      "        [-0.1315,  0.3215,  0.0404,  0.6812,  0.9377, -0.3931, -0.5630],\n",
      "        [-0.3715,  0.5073, -0.0542,  0.8065,  0.9433, -0.4017, -0.4586],\n",
      "        [-0.2651,  0.3954,  0.1232,  0.7295,  1.0724, -0.3110, -0.3172],\n",
      "        [-0.2827,  0.3137,  0.0442,  0.5760,  0.8832, -0.2397, -0.2672],\n",
      "        [-0.1799,  0.1271,  0.0240,  0.6258,  0.8851, -0.4505, -0.4309],\n",
      "        [-0.3661,  0.3316, -0.0499,  0.4315,  0.9078, -0.5075, -0.1557],\n",
      "        [-0.1995,  0.2720,  0.1059,  0.7412,  0.8107, -0.2253, -0.3644],\n",
      "        [-0.1650,  0.3568,  0.0167,  0.7365,  0.9025, -0.4477, -0.5323],\n",
      "        [-0.1471,  0.2714,  0.1255,  0.6972,  0.7669, -0.2158, -0.4615],\n",
      "        [-0.1910,  0.3091,  0.0919,  0.6922,  0.9106, -0.3546, -0.4584],\n",
      "        [-0.1408,  0.3030,  0.0947,  0.6696,  0.8956, -0.3349, -0.3306],\n",
      "        [-0.3381,  0.4065, -0.0297,  0.6664,  0.9214, -0.2632, -0.4722],\n",
      "        [-0.3044,  0.3151, -0.0731,  0.7217,  1.1533, -0.3040, -0.4011],\n",
      "        [-0.0676,  0.2469,  0.0150,  0.6037,  0.6831, -0.2014, -0.3635],\n",
      "        [-0.3216,  0.4086, -0.1023,  0.5974,  0.8783, -0.2741, -0.3019],\n",
      "        [-0.1785,  0.2808,  0.1341,  0.6700,  0.7534, -0.4129, -0.3377],\n",
      "        [-0.3664,  0.4550, -0.1137,  0.5848,  1.0381, -0.4312, -0.3661],\n",
      "        [-0.0852,  0.1656, -0.1370,  0.6583,  0.9721, -0.4650, -0.3425],\n",
      "        [-0.3009,  0.3399,  0.0215,  0.6780,  0.9237, -0.3036, -0.3900],\n",
      "        [-0.1603,  0.3291,  0.0133,  0.6880,  0.8108, -0.4077, -0.3521],\n",
      "        [-0.0310,  0.3320,  0.0877,  0.7107,  0.8786, -0.4089, -0.4143],\n",
      "        [-0.2928,  0.2444,  0.0669,  0.5886,  0.9652, -0.3238, -0.3549],\n",
      "        [-0.0905,  0.2955,  0.1169,  0.7152,  0.8918, -0.3243, -0.2894],\n",
      "        [-0.2481,  0.3157, -0.0074,  0.6663,  0.7976, -0.4169, -0.4273],\n",
      "        [-0.1505,  0.2775,  0.0324,  0.6250,  0.8363, -0.3022, -0.3694],\n",
      "        [-0.1554,  0.1306,  0.1044,  0.5409,  0.8920, -0.2380, -0.3582],\n",
      "        [-0.3731,  0.4363,  0.1292,  0.5618,  0.7319, -0.2311, -0.4656],\n",
      "        [-0.2310,  0.3467,  0.0078,  0.6056,  0.8090, -0.5432, -0.1641],\n",
      "        [-0.1775,  0.3333,  0.0629,  0.5737,  0.9834, -0.4071, -0.2813],\n",
      "        [-0.2346,  0.3475, -0.1428,  0.6586,  0.8325, -0.4236, -0.4205],\n",
      "        [-0.1986,  0.3882,  0.1407,  0.5846,  0.6671, -0.2812, -0.4160],\n",
      "        [-0.0992,  0.1795,  0.1079,  0.6379,  0.7824, -0.2577, -0.2694],\n",
      "        [-0.3231,  0.2908, -0.1213,  0.5546,  0.8349, -0.4455, -0.3242],\n",
      "        [-0.2821,  0.3516,  0.0231,  0.7127,  0.8978, -0.3164, -0.4445],\n",
      "        [-0.0636,  0.2766,  0.0316,  0.5717,  0.7848, -0.4909, -0.3661],\n",
      "        [-0.0819,  0.2897,  0.0767,  0.5355,  0.9107, -0.4205, -0.1828],\n",
      "        [-0.1421,  0.2667,  0.0734,  0.6211,  0.8981, -0.3521, -0.2765],\n",
      "        [-0.2084,  0.3970, -0.1444,  0.6778,  0.8816, -0.4364, -0.3438],\n",
      "        [-0.3794,  0.4437,  0.0067,  0.5987,  0.8654, -0.3707, -0.4427],\n",
      "        [-0.1227,  0.1164,  0.0013,  0.5426,  0.9322, -0.3147, -0.4041],\n",
      "        [-0.2288,  0.2661,  0.0046,  0.6745,  0.8878, -0.3931, -0.2682],\n",
      "        [-0.1179,  0.2080,  0.0866,  0.6647,  0.9925, -0.3034, -0.3891],\n",
      "        [-0.1894,  0.3620,  0.0498,  0.6827,  0.9289, -0.3693, -0.4148],\n",
      "        [-0.1283,  0.1439,  0.0196,  0.6920,  0.7590, -0.1494, -0.4146],\n",
      "        [-0.3055,  0.4611, -0.0485,  0.6716,  0.8630, -0.4089, -0.3125],\n",
      "        [-0.2013,  0.3025,  0.0359,  0.7324,  0.8182, -0.3753, -0.4172],\n",
      "        [-0.2295,  0.3782,  0.0814,  0.4971,  0.8146, -0.3128, -0.3268],\n",
      "        [-0.2262,  0.2929,  0.0716,  0.5802,  0.7799, -0.3032, -0.3771],\n",
      "        [-0.1016,  0.3264,  0.0756,  0.6845,  0.8684, -0.4520, -0.2779],\n",
      "        [-0.0937,  0.2806,  0.0592,  0.6762,  0.8789, -0.2686, -0.4566],\n",
      "        [-0.2183,  0.4238,  0.0217,  0.6734,  0.9450, -0.3571, -0.3639],\n",
      "        [-0.0836,  0.2245,  0.0350,  0.6130,  0.8647, -0.3228, -0.3090],\n",
      "        [-0.2637,  0.3795,  0.0716,  0.6560,  1.0427, -0.3556, -0.3816]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "simple_mlp = SimpleMLP(input_dim=128, output_dim=7)\n",
    "print(simple_mlp)\n",
    "\n",
    "input = out\n",
    "out = simple_mlp(input)\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_encoder = PixelSetEncoder(10, mlp1=[10, 32],\n",
    "                                pooling=\"mean_std\",\n",
    "                                mlp2=[64, 128])\n",
    "\n",
    "# transformer_encoder = TimeSeriesTransformer(feature_size=128)\n",
    "transformer_encoder = TimeSeriesTransformer(d_model=128)\n",
    "\n",
    "simple_mlp = SimpleMLP(input_dim=128, output_dim=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompleteModel(nn.Module):\n",
    "    def __init__(self, pixel_encoder, transformer_encoder, simple_mlp):\n",
    "        super(CompleteModel, self).__init__()\n",
    "        self.pixel_encoder = pixel_encoder\n",
    "        self.transformer_encoder = transformer_encoder\n",
    "        self.simple_mlp = simple_mlp\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        x = self.pixel_encoder(x, mask)  # torch.Size([8, 52, 128])\n",
    "        x = self.transformer_encoder(x)  # torch.Size([8, 128])\n",
    "        x = self.simple_mlp(x) # torch.Size([8, 7])\n",
    "        return x\n",
    "\n",
    "model = CompleteModel(pixel_encoder, transformer_encoder, simple_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δοκιμή Ολόκληρου Μοντέλου για είσοδο από Train Dataset με σταθερό πλήθος pixels = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 52, 10, 32])\n",
      "torch.Size([64])\n",
      "tensor([0, 1, 1, 1, 5, 2, 1, 4, 3, 2, 1, 3, 2, 6, 1, 4, 5, 0, 0, 4, 2, 2, 5, 5,\n",
      "        1, 3, 2, 5, 2, 1, 1, 1, 1, 3, 1, 5, 4, 0, 1, 1, 1, 5, 1, 1, 1, 1, 2, 6,\n",
      "        3, 2, 3, 1, 2, 4, 1, 2, 1, 2, 1, 6, 0, 2, 1, 2])\n",
      "torch.Size([64, 52, 32])\n",
      "\n",
      "torch.Size([64, 7])\n",
      "tensor([[ 0.0673, -0.2916,  0.0263,  0.4562, -0.2787,  0.5332, -0.3714],\n",
      "        [ 0.2055, -0.4213,  0.1092,  0.4613, -0.2087,  0.5456, -0.4857],\n",
      "        [ 0.1898, -0.3632, -0.0169,  0.2836, -0.1463,  0.5887, -0.4524],\n",
      "        [ 0.0788, -0.2758,  0.1793,  0.2031, -0.2136,  0.4173, -0.1908],\n",
      "        [ 0.1343, -0.4454, -0.0020,  0.3815, -0.2549,  0.5937, -0.3633],\n",
      "        [ 0.1074, -0.4615,  0.0320,  0.2680, -0.1209,  0.5287, -0.2691],\n",
      "        [ 0.1954, -0.3636,  0.0434,  0.3860, -0.2647,  0.5393, -0.4162],\n",
      "        [ 0.1652, -0.5126, -0.1073,  0.3853, -0.4215,  0.5480, -0.2626],\n",
      "        [ 0.0869, -0.2835,  0.0684,  0.4643, -0.3630,  0.4841, -0.4420],\n",
      "        [ 0.2171, -0.5031,  0.0430,  0.3907, -0.2656,  0.5736, -0.3644],\n",
      "        [ 0.1156, -0.4005,  0.1076,  0.3931, -0.3288,  0.5075, -0.3427],\n",
      "        [ 0.1284, -0.3099,  0.1531,  0.1566, -0.1547,  0.3114, -0.0400],\n",
      "        [ 0.0416, -0.2992,  0.0651,  0.5517, -0.2129,  0.4773, -0.5150],\n",
      "        [ 0.0526, -0.3644, -0.0250,  0.3868, -0.3531,  0.5635, -0.3442],\n",
      "        [ 0.0175, -0.4555,  0.0756,  0.3073, -0.1657,  0.4808, -0.2099],\n",
      "        [ 0.0192, -0.3426,  0.1830,  0.2434, -0.2953,  0.3344, -0.0697],\n",
      "        [ 0.1923, -0.4368, -0.0385,  0.4787, -0.1336,  0.5515, -0.4797],\n",
      "        [ 0.1037, -0.4016,  0.0246,  0.1859, -0.2573,  0.4325, -0.2568],\n",
      "        [ 0.1116, -0.3988,  0.0852,  0.2392, -0.3075,  0.3583, -0.0934],\n",
      "        [ 0.0558, -0.3003, -0.0317,  0.5411, -0.3455,  0.5938, -0.5576],\n",
      "        [ 0.0449, -0.2944,  0.0144,  0.5859, -0.2783,  0.5792, -0.5015],\n",
      "        [ 0.0215, -0.3148,  0.1022,  0.2468, -0.1938,  0.3841, -0.1611],\n",
      "        [ 0.1312, -0.4156, -0.0436,  0.4519, -0.2979,  0.5831, -0.5047],\n",
      "        [ 0.0740, -0.3508,  0.0216,  0.3891, -0.2776,  0.5291, -0.3478],\n",
      "        [ 0.1513, -0.3823,  0.0938,  0.4004, -0.2086,  0.5109, -0.2718],\n",
      "        [ 0.1481, -0.3290, -0.0135,  0.3738, -0.2536,  0.4015, -0.3952],\n",
      "        [ 0.0155, -0.3065,  0.0630,  0.3715, -0.1022,  0.6027, -0.3599],\n",
      "        [ 0.0958, -0.3997,  0.0138,  0.4490, -0.2614,  0.6857, -0.5157],\n",
      "        [ 0.0711, -0.2895, -0.0391,  0.5256, -0.2149,  0.4849, -0.5481],\n",
      "        [ 0.1891, -0.4881,  0.0592,  0.3009, -0.3128,  0.5857, -0.4500],\n",
      "        [ 0.0877, -0.3002,  0.0183,  0.2132, -0.2796,  0.4155, -0.2666],\n",
      "        [ 0.1250, -0.3704, -0.0884,  0.5271, -0.3226,  0.5431, -0.5954],\n",
      "        [ 0.1430, -0.3920,  0.0150,  0.3801, -0.3205,  0.5919, -0.2821],\n",
      "        [ 0.0438, -0.3865,  0.0719,  0.4240, -0.2543,  0.4180, -0.3236],\n",
      "        [ 0.2172, -0.3589, -0.0031,  0.3609, -0.1644,  0.5245, -0.3816],\n",
      "        [ 0.0893, -0.2349,  0.0694,  0.4630, -0.4013,  0.4406, -0.3593],\n",
      "        [ 0.1530, -0.4575, -0.0925,  0.4695, -0.2508,  0.6109, -0.6244],\n",
      "        [ 0.0244, -0.3774, -0.0682,  0.4251, -0.3104,  0.4750, -0.3543],\n",
      "        [ 0.0445, -0.3821,  0.0745,  0.3618, -0.2893,  0.4735, -0.2552],\n",
      "        [ 0.0342, -0.2960,  0.0215,  0.3171, -0.3648,  0.4876, -0.3048],\n",
      "        [ 0.0728, -0.3507,  0.1098,  0.4592, -0.4019,  0.4816, -0.3315],\n",
      "        [ 0.1253, -0.3304,  0.0524,  0.5366, -0.3182,  0.4845, -0.3512],\n",
      "        [ 0.0587, -0.4577,  0.0097,  0.5638, -0.2755,  0.6142, -0.5530],\n",
      "        [ 0.1089, -0.4310,  0.0662,  0.4072, -0.2020,  0.4801, -0.4346],\n",
      "        [ 0.0097, -0.3529,  0.0693,  0.4533, -0.2904,  0.5108, -0.4028],\n",
      "        [ 0.0505, -0.3650,  0.0312,  0.4244, -0.2906,  0.5866, -0.3804],\n",
      "        [ 0.1238, -0.4940,  0.0819,  0.3178, -0.2204,  0.6153, -0.2155],\n",
      "        [ 0.1042, -0.3101,  0.0829,  0.4767, -0.2365,  0.4914, -0.3303],\n",
      "        [ 0.1804, -0.4478, -0.0516,  0.2755, -0.3032,  0.5154, -0.2496],\n",
      "        [ 0.0105, -0.2679,  0.0409,  0.4823, -0.2721,  0.5090, -0.3790],\n",
      "        [ 0.2468, -0.2673,  0.0951,  0.3660, -0.1787,  0.5493, -0.4878],\n",
      "        [ 0.2445, -0.3914,  0.1223,  0.3681, -0.3610,  0.4380, -0.3841],\n",
      "        [ 0.0401, -0.3769, -0.0621,  0.4991, -0.2777,  0.5633, -0.5079],\n",
      "        [ 0.0307, -0.3393, -0.0343,  0.4604, -0.3569,  0.4599, -0.4019],\n",
      "        [ 0.1885, -0.3478,  0.1040,  0.2976, -0.3047,  0.3633, -0.3550],\n",
      "        [ 0.0845, -0.3770,  0.0465,  0.4583, -0.1862,  0.5385, -0.4431],\n",
      "        [ 0.1027, -0.3440,  0.0246,  0.3761, -0.3630,  0.4795, -0.3871],\n",
      "        [-0.0308, -0.3861, -0.0304,  0.4015, -0.3075,  0.6428, -0.2925],\n",
      "        [ 0.0785, -0.3671,  0.0729,  0.3121, -0.2164,  0.4958, -0.1033],\n",
      "        [ 0.1182, -0.3216,  0.0554,  0.3156, -0.3723,  0.5588, -0.2774],\n",
      "        [ 0.0397, -0.3401,  0.0625,  0.2484, -0.2644,  0.3917, -0.2167],\n",
      "        [ 0.0164, -0.2802, -0.0415,  0.4886, -0.2803,  0.4894, -0.4323],\n",
      "        [ 0.1197, -0.3745,  0.0825,  0.4151, -0.3405,  0.5819, -0.4250],\n",
      "        [ 0.0105, -0.3869,  0.1406,  0.2280, -0.3110,  0.4020, -0.0828]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_dloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=pad_sequences_collate_fn,\n",
    "    # num_workers=4,\n",
    ")\n",
    "\n",
    "for batch in train_dloader:\n",
    "    input_tensor = batch[0]\n",
    "    labels = batch[1]\n",
    "    mask = batch[2]\n",
    "\n",
    "    print(input_tensor.shape)\n",
    "    print(labels.shape)\n",
    "    print(labels)\n",
    "    print(mask.shape)\n",
    "    print()\n",
    "\n",
    "    break\n",
    "\n",
    "out = model(input_tensor, mask)\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δοκιμή Ολόκληρου Μοντέλου για είσοδο από Val Dataset με μεταβλητό πλήθος pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 52, 10, 984])\n",
      "torch.Size([8])\n",
      "tensor([2, 1, 1, 3, 0, 2, 1, 6])\n",
      "torch.Size([8, 52, 984])\n",
      "\n",
      "torch.Size([8, 7])\n",
      "tensor([[ 4.2552e-02, -2.4886e-01,  1.3728e-03,  5.4222e-01, -3.4338e-01,\n",
      "          3.8799e-01, -4.0346e-01],\n",
      "        [ 2.3111e-01, -3.9134e-01,  2.2441e-05,  4.1829e-01, -1.2590e-01,\n",
      "          5.0973e-01, -5.2978e-01],\n",
      "        [ 7.5253e-02, -3.8004e-01,  1.1530e-01,  3.4886e-01, -2.7648e-01,\n",
      "          4.5949e-01, -2.7855e-01],\n",
      "        [ 2.0189e-01, -3.2991e-01,  9.9848e-02,  4.5252e-01, -2.3260e-01,\n",
      "          5.3377e-01, -5.2884e-01],\n",
      "        [ 7.2022e-02, -4.5869e-01, -1.4452e-01,  4.5378e-01, -2.5426e-01,\n",
      "          4.3806e-01, -3.8581e-01],\n",
      "        [ 1.3725e-01, -3.6623e-01,  1.0538e-01,  3.0821e-01, -3.3143e-01,\n",
      "          4.0145e-01, -2.3071e-01],\n",
      "        [ 1.2191e-01, -3.4449e-01,  9.2791e-02,  2.6776e-01, -3.2138e-01,\n",
      "          4.7177e-01, -3.4768e-01],\n",
      "        [ 1.1603e-01, -1.8740e-01,  7.6889e-02,  3.5534e-01, -3.4639e-01,\n",
      "          4.8111e-01, -4.8802e-01]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch in val_dloader:\n",
    "    input_tensor = batch[0]\n",
    "    labels = batch[1]\n",
    "    mask = batch[2]\n",
    "\n",
    "    print(input_tensor.shape)\n",
    "    print(labels.shape)\n",
    "    print(labels)\n",
    "    print(mask.shape)\n",
    "    print()\n",
    "\n",
    "    break\n",
    "\n",
    "out = model(input_tensor, mask)\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
